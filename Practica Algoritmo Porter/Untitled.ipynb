{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a4900ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MARTIN_EXTENSIONS',\n",
       " 'NLTK_EXTENSIONS',\n",
       " 'ORIGINAL_ALGORITHM',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_apply_rule_list',\n",
       " '_contains_vowel',\n",
       " '_ends_cvc',\n",
       " '_ends_double_consonant',\n",
       " '_has_positive_measure',\n",
       " '_is_consonant',\n",
       " '_measure',\n",
       " '_replace_suffix',\n",
       " '_step1a',\n",
       " '_step1b',\n",
       " '_step1c',\n",
       " '_step2',\n",
       " '_step3',\n",
       " '_step4',\n",
       " '_step5a',\n",
       " '_step5b',\n",
       " 'mode',\n",
       " 'pool',\n",
       " 'stem',\n",
       " 'vowels']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "dir(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9c337b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coder\n",
      "code\n",
      "code\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('coder'))\n",
    "print(ps.stem('coding'))\n",
    "print(ps.stem('code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc815cbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m archivo_texto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtexto.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m new_text \u001b[38;5;241m=\u001b[39m archivo_texto\n\u001b[1;32m----> 9\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ps\u001b[38;5;241m.\u001b[39mstem(w))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1321\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1421\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1422\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    316\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m   1394\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1395\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match_potential_end_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1396\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1397\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1375\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1373\u001b[0m before_words \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1374\u001b[0m matches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)):\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;66;03m# Ignore matches that have already been captured by matches to the right of this match\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m matches \u001b[38;5;129;01mand\u001b[39;00m match\u001b[38;5;241m.\u001b[39mend() \u001b[38;5;241m>\u001b[39m before_start:\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "\n",
    "archivo_texto = open(\"texto.txt\", \"w\")\n",
    "\n",
    "new_text = archivo_texto\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "006deb64",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m archivo_texto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtexto.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m new_text \u001b[38;5;241m=\u001b[39m archivo_texto\n\u001b[1;32m----> 9\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ps\u001b[38;5;241m.\u001b[39mstem(w))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1321\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1421\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1422\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    316\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m   1394\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1395\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match_potential_end_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1396\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1397\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1375\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1373\u001b[0m before_words \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1374\u001b[0m matches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)):\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;66;03m# Ignore matches that have already been captured by matches to the right of this match\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m matches \u001b[38;5;129;01mand\u001b[39;00m match\u001b[38;5;241m.\u001b[39mend() \u001b[38;5;241m>\u001b[39m before_start:\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "\n",
    "archivo_texto = open(\"texto.txt\", \"r\")\n",
    "\n",
    "new_text = archivo_texto\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2180c7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "\n",
    "archivo_texto = open(\"texto2.txt\", \"r\")\n",
    "new_text = archivo_texto.read()\n",
    "\n",
    "print(new_text)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "'''\n",
    "archivo_texto.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e295cd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nwords = word_tokenize(new_text)\\nfor w in words:\\n    print(ps.stem(w))\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "\n",
    "archivo_texto = open(\"texto2.txt\", \"r\")\n",
    "new_text = archivo_texto.read()\n",
    "\n",
    "print(new_text)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81cbdb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "\n",
    "archivo_texto = open(\"texto2.txt\", \"r\")\n",
    "new_text = archivo_texto.read()\n",
    "\n",
    "print(new_text)\n",
    "\n",
    "\n",
    "archivo_texto.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "731d83da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onc\n",
      "upon\n",
      "a\n",
      "time\n",
      "there\n",
      "live\n",
      "in\n",
      "a\n",
      "certain\n",
      "villag\n",
      "a\n",
      "littl\n",
      "countri\n",
      "girl\n",
      ",\n",
      "the\n",
      "prettiest\n",
      "creatur\n",
      "who\n",
      "wa\n",
      "ever\n",
      "seen\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "\n",
    "archivo_texto = open(\"texto2.txt\", \"r\")\n",
    "new_text = archivo_texto.read()\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "#archivo_texto.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a227dd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "érase\n",
      "una\n",
      "vez\n",
      "una\n",
      "niñita\n",
      "que\n",
      "lucía\n",
      "una\n",
      "hermosa\n",
      "capa\n",
      "de\n",
      "color\n",
      "rojo\n",
      ".\n",
      "como\n",
      "la\n",
      "niña\n",
      "la\n",
      "usaba\n",
      "muy\n",
      "a\n",
      "menudo\n",
      ",\n",
      "todo\n",
      "la\n",
      "llamaban\n",
      "caperucita\n",
      "roja\n",
      ".\n",
      "un\n",
      "día\n",
      ",\n",
      "la\n",
      "mamá\n",
      "de\n",
      "caperucita\n",
      "roja\n",
      "la\n",
      "llamó\n",
      "y\n",
      "le\n",
      "dijo\n",
      ":\n",
      "—abuelita\n",
      "no\n",
      "se\n",
      "sient\n",
      "muy\n",
      "bien\n",
      ",\n",
      "he\n",
      "horneado\n",
      "una\n",
      "galletica\n",
      "y\n",
      "quiero\n",
      "que\n",
      "tú\n",
      "se\n",
      "la\n",
      "lleve\n",
      ".\n",
      "—claro\n",
      "que\n",
      "sí\n",
      "—respondió\n",
      "caperucita\n",
      "roja\n",
      ",\n",
      "poniéndos\n",
      "su\n",
      "capa\n",
      "y\n",
      "llenando\n",
      "su\n",
      "canasta\n",
      "de\n",
      "galletica\n",
      "recién\n",
      "horneada\n",
      ".\n",
      "ant\n",
      "de\n",
      "salir\n",
      ",\n",
      "su\n",
      "mamá\n",
      "le\n",
      "dijo\n",
      ":\n",
      "—\n",
      "escúcham\n",
      "muy\n",
      "bien\n",
      ",\n",
      "quédat\n",
      "en\n",
      "el\n",
      "camino\n",
      "y\n",
      "nunca\n",
      "habl\n",
      "con\n",
      "extraño\n",
      ".\n",
      "—yo\n",
      "sé\n",
      "mamá\n",
      "—respondió\n",
      "caperucita\n",
      "roja\n",
      "y\n",
      "salió\n",
      "inmediatament\n",
      "hacia\n",
      "la\n",
      "casa\n",
      "de\n",
      "la\n",
      "abuelita\n",
      ".\n",
      "para\n",
      "llegar\n",
      "a\n",
      "casa\n",
      "de\n",
      "la\n",
      "abuelita\n",
      ",\n",
      "caperucita\n",
      "debía\n",
      "atravesar\n",
      "un\n",
      "camino\n",
      "a\n",
      "lo\n",
      "largo\n",
      "del\n",
      "espeso\n",
      "bosqu\n",
      ".\n",
      "en\n",
      "el\n",
      "camino\n",
      ",\n",
      "se\n",
      "encontró\n",
      "con\n",
      "el\n",
      "lobo\n",
      ".\n",
      "—hola\n",
      "niñita\n",
      ",\n",
      "¿hacia\n",
      "dónde\n",
      "te\n",
      "dirig\n",
      "en\n",
      "est\n",
      "maravilloso\n",
      "día\n",
      "?\n",
      "—preguntó\n",
      "el\n",
      "lobo\n",
      ".\n",
      "caperucita\n",
      "roja\n",
      "recordó\n",
      "que\n",
      "su\n",
      "mamá\n",
      "le\n",
      "había\n",
      "advertido\n",
      "no\n",
      "hablar\n",
      "con\n",
      "extraño\n",
      ",\n",
      "pero\n",
      "el\n",
      "lobo\n",
      "lucía\n",
      "muy\n",
      "elegant\n",
      ",\n",
      "ademá\n",
      "era\n",
      "muy\n",
      "amig\n",
      "y\n",
      "educado\n",
      ".\n",
      "—voy\n",
      "a\n",
      "la\n",
      "casa\n",
      "de\n",
      "abuelita\n",
      ",\n",
      "señor\n",
      "lobo\n",
      "—respondió\n",
      "la\n",
      "niña—\n",
      ".\n",
      "ella\n",
      "se\n",
      "encuentra\n",
      "enferma\n",
      "y\n",
      "voy\n",
      "a\n",
      "llevarl\n",
      "esta\n",
      "galletica\n",
      "para\n",
      "animarla\n",
      "un\n",
      "poco\n",
      ".\n",
      "—¡qué\n",
      "buena\n",
      "niña\n",
      "ere\n",
      "!\n",
      "—exclamó\n",
      "el\n",
      "lobo\n",
      ".\n",
      "—¿qué\n",
      "tan\n",
      "lejo\n",
      "tien\n",
      "que\n",
      "ir\n",
      "?\n",
      "—¡oh\n",
      "!\n",
      "debo\n",
      "llegar\n",
      "hasta\n",
      "el\n",
      "final\n",
      "del\n",
      "camino\n",
      ",\n",
      "ahí\n",
      "vive\n",
      "abuelita—dijo\n",
      "caperucita\n",
      "con\n",
      "una\n",
      "sonrisa\n",
      ".\n",
      "—te\n",
      "deseo\n",
      "un\n",
      "muy\n",
      "feliz\n",
      "día\n",
      "mi\n",
      "niña\n",
      "—respondió\n",
      "el\n",
      "lobo\n",
      ".\n",
      "el\n",
      "lobo\n",
      "se\n",
      "adentró\n",
      "en\n",
      "el\n",
      "bosqu\n",
      ".\n",
      "él\n",
      "tenía\n",
      "un\n",
      "enorm\n",
      "apetito\n",
      "y\n",
      "en\n",
      "realidad\n",
      "no\n",
      "era\n",
      "de\n",
      "confiar\n",
      ".\n",
      "así\n",
      "que\n",
      "corrió\n",
      "hasta\n",
      "la\n",
      "casa\n",
      "de\n",
      "la\n",
      "abuela\n",
      "ant\n",
      "de\n",
      "que\n",
      "caperucita\n",
      "pudiera\n",
      "alcanzarlo\n",
      ".\n",
      "su\n",
      "plan\n",
      "era\n",
      "comers\n",
      "a\n",
      "la\n",
      "abuela\n",
      ",\n",
      "a\n",
      "caperucita\n",
      "roja\n",
      "y\n",
      "a\n",
      "toda\n",
      "la\n",
      "galletica\n",
      "recién\n",
      "horneada\n",
      ".\n",
      "el\n",
      "lobo\n",
      "tocó\n",
      "la\n",
      "puerta\n",
      "de\n",
      "la\n",
      "abuela\n",
      ".\n",
      "al\n",
      "verlo\n",
      ",\n",
      "la\n",
      "abuelita\n",
      "corrió\n",
      "despavorida\n",
      "dejando\n",
      "atrá\n",
      "su\n",
      "chal\n",
      ".\n",
      "el\n",
      "lobo\n",
      "tomó\n",
      "el\n",
      "chal\n",
      "de\n",
      "la\n",
      "viejecita\n",
      "y\n",
      "luego\n",
      "se\n",
      "puso\n",
      "su\n",
      "lent\n",
      "y\n",
      "su\n",
      "gorrito\n",
      "de\n",
      "noch\n",
      ".\n",
      "rápidament\n",
      ",\n",
      "se\n",
      "trepó\n",
      "en\n",
      "la\n",
      "cama\n",
      "de\n",
      "la\n",
      "abuelita\n",
      ",\n",
      "cubriéndos\n",
      "hasta\n",
      "la\n",
      "nariz\n",
      "con\n",
      "la\n",
      "manta\n",
      ".\n",
      "pronto\n",
      "escuchó\n",
      "que\n",
      "tocaban\n",
      "la\n",
      "puerta\n",
      ":\n",
      "—abuelita\n",
      ",\n",
      "soy\n",
      "yo\n",
      ",\n",
      "caperucita\n",
      "roja\n",
      ".\n",
      "con\n",
      "vo\n",
      "disimulada\n",
      ",\n",
      "tratando\n",
      "de\n",
      "sonar\n",
      "como\n",
      "la\n",
      "abuelita\n",
      ",\n",
      "el\n",
      "lobo\n",
      "dijo\n",
      ":\n",
      "—pasa\n",
      "mi\n",
      "niña\n",
      ",\n",
      "estoy\n",
      "en\n",
      "camita\n",
      ".\n",
      "caperucita\n",
      "roja\n",
      "pensó\n",
      "que\n",
      "su\n",
      "abuelita\n",
      "se\n",
      "encontraba\n",
      "muy\n",
      "enferma\n",
      "porqu\n",
      "se\n",
      "veía\n",
      "muy\n",
      "pálida\n",
      "y\n",
      "sonaba\n",
      "terribl\n",
      ".\n",
      "—¡abuelita\n",
      ",\n",
      "abuelita\n",
      ",\n",
      "qué\n",
      "ojo\n",
      "má\n",
      "grand\n",
      "tien\n",
      "!\n",
      "—son\n",
      "para\n",
      "vert\n",
      "mejor\n",
      "—respondió\n",
      "el\n",
      "lobo\n",
      ".\n",
      "—¡abuelita\n",
      ",\n",
      "abuelita\n",
      ",\n",
      "qué\n",
      "oreja\n",
      "má\n",
      "grand\n",
      "tien\n",
      "!\n",
      "—son\n",
      "para\n",
      "oírt\n",
      "mejor\n",
      "—susurró\n",
      "el\n",
      "lobo\n",
      ".\n",
      "—¡abuelita\n",
      ",\n",
      "abuelita\n",
      ",\n",
      "que\n",
      "dient\n",
      "má\n",
      "grand\n",
      "tien\n",
      "!\n",
      "—¡son\n",
      "para\n",
      "comert\n",
      "mejor\n",
      "!\n",
      "con\n",
      "esta\n",
      "palabra\n",
      ",\n",
      "el\n",
      "malvado\n",
      "lobo\n",
      "tiró\n",
      "su\n",
      "manta\n",
      "y\n",
      "saltó\n",
      "de\n",
      "la\n",
      "cama\n",
      ".\n",
      "asustada\n",
      ",\n",
      "caperucita\n",
      "salió\n",
      "corriendo\n",
      "hacia\n",
      "la\n",
      "puerta\n",
      ".\n",
      "justo\n",
      "en\n",
      "ese\n",
      "momento\n",
      ",\n",
      "un\n",
      "leñador\n",
      "se\n",
      "acercó\n",
      "a\n",
      "la\n",
      "puerta\n",
      ",\n",
      "la\n",
      "cual\n",
      "se\n",
      "encontraba\n",
      "entreabierta\n",
      ".\n",
      "la\n",
      "abuelita\n",
      "estaba\n",
      "escondida\n",
      "detrá\n",
      "de\n",
      "él\n",
      ".\n",
      "al\n",
      "ver\n",
      "al\n",
      "leñador\n",
      ",\n",
      "el\n",
      "lobo\n",
      "saltó\n",
      "por\n",
      "la\n",
      "ventana\n",
      "y\n",
      "huyó\n",
      "espantado\n",
      "para\n",
      "nunca\n",
      "ser\n",
      "visto\n",
      ".\n",
      "la\n",
      "abuelita\n",
      "y\n",
      "caperucita\n",
      "roja\n",
      "agradecieron\n",
      "al\n",
      "leñador\n",
      "por\n",
      "salvarla\n",
      "del\n",
      "malvado\n",
      "lobo\n",
      "y\n",
      "todo\n",
      "comieron\n",
      "galletica\n",
      "con\n",
      "lech\n",
      ".\n",
      "ese\n",
      "día\n",
      "caperucita\n",
      "roja\n",
      "aprendió\n",
      "una\n",
      "important\n",
      "lección\n",
      ":\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "\n",
    "archivo_texto = open(\"texto.txt\", \"r\", encoding=\"utf8\")\n",
    "new_text = archivo_texto.read()\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "archivo_texto.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b75609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onc\n",
      "upon\n",
      "a\n",
      "time\n",
      "there\n",
      "live\n",
      "in\n",
      "a\n",
      "certain\n",
      "villag\n",
      "a\n",
      "littl\n",
      "countri\n",
      "girl\n",
      ",\n",
      "the\n",
      "prettiest\n",
      "creatur\n",
      "who\n",
      "wa\n",
      "ever\n",
      "seen\n",
      ".\n",
      "her\n",
      "mother\n",
      "wa\n",
      "excess\n",
      "fond\n",
      "of\n",
      "her\n",
      ";\n",
      "and\n",
      "her\n",
      "grandmoth\n",
      "dote\n",
      "on\n",
      "her\n",
      "still\n",
      "more\n",
      ".\n",
      "thi\n",
      "good\n",
      "woman\n",
      "had\n",
      "a\n",
      "littl\n",
      "red\n",
      "ride\n",
      "hood\n",
      "made\n",
      "for\n",
      "her\n",
      ".\n",
      "it\n",
      "suit\n",
      "the\n",
      "girl\n",
      "so\n",
      "extrem\n",
      "well\n",
      "that\n",
      "everybodi\n",
      "call\n",
      "her\n",
      "littl\n",
      "red\n",
      "ride\n",
      "hood\n",
      ".\n",
      "one\n",
      "day\n",
      "her\n",
      "mother\n",
      ",\n",
      "have\n",
      "made\n",
      "some\n",
      "cake\n",
      ",\n",
      "said\n",
      "to\n",
      "her\n",
      ",\n",
      "“\n",
      "go\n",
      ",\n",
      "my\n",
      "dear\n",
      ",\n",
      "and\n",
      "see\n",
      "how\n",
      "your\n",
      "grandmoth\n",
      "is\n",
      "do\n",
      ",\n",
      "for\n",
      "i\n",
      "hear\n",
      "she\n",
      "ha\n",
      "been\n",
      "veri\n",
      "ill.\n",
      "take\n",
      "her\n",
      "a\n",
      "cake\n",
      ",\n",
      "and\n",
      "thi\n",
      "littl\n",
      "pot\n",
      "of\n",
      "butter\n",
      ".\n",
      "littl\n",
      "red\n",
      "ride\n",
      "hood\n",
      "set\n",
      "out\n",
      "immedi\n",
      "to\n",
      "go\n",
      "to\n",
      "her\n",
      "grandmoth\n",
      ",\n",
      "who\n",
      "live\n",
      "in\n",
      "anoth\n",
      "villag\n",
      ".\n",
      "as\n",
      "she\n",
      "wa\n",
      "go\n",
      "through\n",
      "the\n",
      "wood\n",
      ",\n",
      "she\n",
      "met\n",
      "with\n",
      "a\n",
      "wolf\n",
      ",\n",
      "who\n",
      "had\n",
      "a\n",
      "veri\n",
      "great\n",
      "mind\n",
      "to\n",
      "eat\n",
      "her\n",
      "up\n",
      ",\n",
      "but\n",
      "he\n",
      "dare\n",
      "not\n",
      ",\n",
      "becaus\n",
      "of\n",
      "some\n",
      "woodcutt\n",
      "work\n",
      "nearbi\n",
      "in\n",
      "the\n",
      "forest\n",
      ".\n",
      "he\n",
      "ask\n",
      "her\n",
      "where\n",
      "she\n",
      "wa\n",
      "go\n",
      ".\n",
      "the\n",
      "poor\n",
      "child\n",
      ",\n",
      "who\n",
      "did\n",
      "not\n",
      "know\n",
      "that\n",
      "it\n",
      "wa\n",
      "danger\n",
      "to\n",
      "stay\n",
      "and\n",
      "talk\n",
      "to\n",
      "a\n",
      "wolf\n",
      ",\n",
      "said\n",
      "to\n",
      "him\n",
      ",\n",
      "“\n",
      "i\n",
      "am\n",
      "go\n",
      "to\n",
      "see\n",
      "my\n",
      "grandmoth\n",
      "and\n",
      "carri\n",
      "her\n",
      "a\n",
      "cake\n",
      "and\n",
      "a\n",
      "littl\n",
      "pot\n",
      "of\n",
      "butter\n",
      "from\n",
      "my\n",
      "mother.\n",
      "”\n",
      "“\n",
      "doe\n",
      "she\n",
      "live\n",
      "far\n",
      "off\n",
      "?\n",
      "”\n",
      "said\n",
      "the\n",
      "wolf\n",
      "``\n",
      "“\n",
      "ye\n",
      ",\n",
      "”\n",
      "answer\n",
      "littl\n",
      "red\n",
      "ride\n",
      "hood\n",
      ";\n",
      "“\n",
      "it\n",
      "is\n",
      "beyond\n",
      "that\n",
      "mill\n",
      "you\n",
      "see\n",
      "there\n",
      ",\n",
      "at\n",
      "the\n",
      "first\n",
      "hous\n",
      "in\n",
      "the\n",
      "village.\n",
      "”\n",
      "“\n",
      "well\n",
      ",\n",
      "”\n",
      "said\n",
      "the\n",
      "wolf\n",
      ",\n",
      "“\n",
      "i\n",
      "'ll\n",
      "go\n",
      "to\n",
      "see\n",
      "her\n",
      "too\n",
      ".\n",
      "–\n",
      "i\n",
      "'ll\n",
      "go\n",
      "thi\n",
      "way\n",
      "and\n",
      "go\n",
      "you\n",
      "that\n",
      ",\n",
      "and\n",
      "we\n",
      "shall\n",
      "see\n",
      "who\n",
      "will\n",
      "be\n",
      "there\n",
      "first.\n",
      "”\n",
      "the\n",
      "wolf\n",
      "ran\n",
      "as\n",
      "fast\n",
      "as\n",
      "he\n",
      "could\n",
      ",\n",
      "take\n",
      "the\n",
      "shortest\n",
      "path\n",
      ",\n",
      "and\n",
      "the\n",
      "littl\n",
      "girl\n",
      "took\n",
      "a\n",
      "roundabout\n",
      "way\n",
      ",\n",
      "entertain\n",
      "herself\n",
      "by\n",
      "gather\n",
      "nut\n",
      ",\n",
      "run\n",
      "after\n",
      "butterfli\n",
      ",\n",
      "and\n",
      "gather\n",
      "bouquet\n",
      "of\n",
      "littl\n",
      "flower\n",
      ".\n",
      "it\n",
      "wa\n",
      "not\n",
      "long\n",
      "befor\n",
      "the\n",
      "wolf\n",
      "arriv\n",
      "at\n",
      "the\n",
      "old\n",
      "woman\n",
      "'s\n",
      "hous\n",
      ".\n",
      "he\n",
      "knock\n",
      "at\n",
      "the\n",
      "door\n",
      ":\n",
      "tap\n",
      ",\n",
      "tap\n",
      ".\n",
      "“\n",
      "who\n",
      "'s\n",
      "there\n",
      "?\n",
      "”\n",
      "“\n",
      "your\n",
      "grandchild\n",
      ",\n",
      "littl\n",
      "red\n",
      "ride\n",
      "hood\n",
      ",\n",
      "”\n",
      "repli\n",
      "the\n",
      "wolf\n",
      ",\n",
      "counterfeit\n",
      "her\n",
      "voic\n",
      ";\n",
      "“\n",
      "who\n",
      "ha\n",
      "brought\n",
      "you\n",
      "a\n",
      "cake\n",
      "and\n",
      "a\n",
      "littl\n",
      "pot\n",
      "of\n",
      "butter\n",
      "sent\n",
      "you\n",
      "by\n",
      "mother.\n",
      "”\n",
      "the\n",
      "good\n",
      "grandmoth\n",
      ",\n",
      "who\n",
      "wa\n",
      "in\n",
      "bed\n",
      ",\n",
      "becaus\n",
      "she\n",
      "wa\n",
      "somewhat\n",
      "ill\n",
      ",\n",
      "cri\n",
      "out\n",
      ",\n",
      "“\n",
      "pull\n",
      "the\n",
      "string\n",
      ",\n",
      "and\n",
      "the\n",
      "latch\n",
      "will\n",
      "go\n",
      "up.\n",
      "”\n",
      "the\n",
      "wolf\n",
      "pull\n",
      "the\n",
      "string\n",
      ",\n",
      "and\n",
      "the\n",
      "door\n",
      "open\n",
      ",\n",
      "and\n",
      "then\n",
      "he\n",
      "immedi\n",
      "fell\n",
      "upon\n",
      "the\n",
      "good\n",
      "woman\n",
      "and\n",
      "ate\n",
      "her\n",
      "up\n",
      "in\n",
      "a\n",
      "moment\n",
      ",\n",
      "for\n",
      "it\n",
      "been\n",
      "more\n",
      "than\n",
      "three\n",
      "day\n",
      "sinc\n",
      "he\n",
      "had\n",
      "eaten\n",
      ".\n",
      "he\n",
      "then\n",
      "shut\n",
      "the\n",
      "door\n",
      "and\n",
      "got\n",
      "into\n",
      "the\n",
      "grandmoth\n",
      "'s\n",
      "bed\n",
      ",\n",
      "expect\n",
      "littl\n",
      "red\n",
      "ride\n",
      "hood\n",
      ",\n",
      "who\n",
      "came\n",
      "some\n",
      "time\n",
      "afterward\n",
      "and\n",
      "knock\n",
      "at\n",
      "the\n",
      "door\n",
      ":\n",
      "tap\n",
      ",\n",
      "tap\n",
      ".\n",
      "“\n",
      "who\n",
      "'s\n",
      "there\n",
      "?\n",
      "”\n",
      "littl\n",
      "red\n",
      "ride\n",
      "hood\n",
      ",\n",
      "hear\n",
      "the\n",
      "big\n",
      "voic\n",
      "of\n",
      "the\n",
      "wolf\n",
      ",\n",
      "wa\n",
      "at\n",
      "first\n",
      "afraid\n",
      ";\n",
      "but\n",
      "believ\n",
      "her\n",
      "grandmoth\n",
      "had\n",
      "a\n",
      "cold\n",
      "and\n",
      "wa\n",
      "hoars\n",
      ",\n",
      "answer\n",
      ",\n",
      "“\n",
      "it\n",
      "is\n",
      "your\n",
      "grandchild\n",
      "littl\n",
      "red\n",
      "ride\n",
      "hood\n",
      ",\n",
      "who\n",
      "ha\n",
      "brought\n",
      "you\n",
      "a\n",
      "cake\n",
      "and\n",
      "a\n",
      "littl\n",
      "pot\n",
      "of\n",
      "butter\n",
      "mother\n",
      "send\n",
      "you\n",
      ".\n",
      "the\n",
      "wolf\n",
      "cri\n",
      "out\n",
      "to\n",
      "her\n",
      ",\n",
      "soften\n",
      "hi\n",
      "voic\n",
      "as\n",
      "much\n",
      "as\n",
      "he\n",
      "could\n",
      ",\n",
      "“\n",
      "pull\n",
      "the\n",
      "string\n",
      ",\n",
      "and\n",
      "the\n",
      "latch\n",
      "will\n",
      "go\n",
      "up.\n",
      "”\n",
      "littl\n",
      "red\n",
      "ride\n",
      "hood\n",
      "pull\n",
      "the\n",
      "string\n",
      ",\n",
      "and\n",
      "the\n",
      "door\n",
      "open\n",
      ".\n",
      "the\n",
      "wolf\n",
      ",\n",
      "see\n",
      "her\n",
      "come\n",
      "in\n",
      ",\n",
      "said\n",
      "to\n",
      "her\n",
      ",\n",
      "hide\n",
      "himself\n",
      "under\n",
      "the\n",
      "bedcloth\n",
      ",\n",
      "“\n",
      "put\n",
      "the\n",
      "cake\n",
      "and\n",
      "the\n",
      "littl\n",
      "pot\n",
      "of\n",
      "butter\n",
      "upon\n",
      "the\n",
      "stool\n",
      ",\n",
      "and\n",
      "come\n",
      "sit\n",
      "on\n",
      "the\n",
      "bed\n",
      "with\n",
      "me.\n",
      "”\n",
      "littl\n",
      "red\n",
      "ride\n",
      "hood\n",
      "sat\n",
      "on\n",
      "the\n",
      "bed\n",
      ".\n",
      "she\n",
      "wa\n",
      "greatli\n",
      "amaz\n",
      "to\n",
      "see\n",
      "how\n",
      "her\n",
      "grandmoth\n",
      "in\n",
      "her\n",
      "nightcloth\n",
      ",\n",
      "and\n",
      "said\n",
      "to\n",
      "her\n",
      ",\n",
      "“\n",
      "grandmoth\n",
      ",\n",
      "what\n",
      "big\n",
      "arm\n",
      "you\n",
      "have\n",
      "!\n",
      "”\n",
      "“\n",
      "they\n",
      "are\n",
      "to\n",
      "hug\n",
      "you\n",
      "better\n",
      "..\n",
      "my\n",
      "dear\n",
      "”\n",
      "“\n",
      "grandmoth\n",
      ",\n",
      "what\n",
      "big\n",
      "leg\n",
      "you\n",
      "have\n",
      "!\n",
      "”\n",
      "“\n",
      "they\n",
      "are\n",
      "to\n",
      "run\n",
      "better\n",
      ",\n",
      "my\n",
      "child.\n",
      "”\n",
      "“\n",
      "grandmoth\n",
      ",\n",
      "what\n",
      "big\n",
      "ear\n",
      "you\n",
      "have\n",
      "!\n",
      "”\n",
      "“\n",
      "they\n",
      "are\n",
      "to\n",
      "hear\n",
      "better\n",
      ",\n",
      "my\n",
      "child.\n",
      "”\n",
      "“\n",
      "grandmoth\n",
      ",\n",
      "what\n",
      "big\n",
      "eye\n",
      "you\n",
      "have\n",
      "!\n",
      "”\n",
      "“\n",
      "they\n",
      "are\n",
      "better\n",
      "to\n",
      "see\n",
      "with\n",
      ",\n",
      "my\n",
      "child.\n",
      "”\n",
      "“\n",
      "grandmoth\n",
      ",\n",
      "what\n",
      "big\n",
      "teeth\n",
      "you\n",
      "have\n",
      "got\n",
      "!\n",
      "”\n",
      "“\n",
      "they\n",
      "are\n",
      "to\n",
      "eat\n",
      "you\n",
      "up\n",
      "!\n",
      "!\n",
      ".\n",
      "”\n",
      "and\n",
      ",\n",
      "say\n",
      "these\n",
      "word\n",
      ",\n",
      "thi\n",
      "wick\n",
      "wolf\n",
      "fell\n",
      "upon\n",
      "littl\n",
      "red\n",
      "ride\n",
      "hood\n",
      ",\n",
      "and\n",
      "ate\n",
      "her\n",
      "all\n",
      "up\n",
      ".\n",
      "but\n",
      "a\n",
      "woodcutt\n",
      "went\n",
      "into\n",
      "the\n",
      "hous\n",
      "and\n",
      "take\n",
      "the\n",
      "littl\n",
      "red\n",
      "ride\n",
      "hood\n",
      "and\n",
      "the\n",
      "grandmoth\n",
      "out\n",
      "the\n",
      "wolf\n",
      "'s\n",
      "belli\n",
      ",\n",
      "and\n",
      "the\n",
      "littl\n",
      "red\n",
      "ride\n",
      "hood\n",
      "learn\n",
      "the\n",
      "lesson\n",
      "and\n",
      "live\n",
      "happili\n",
      "''\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "\n",
    "archivo_texto = open(\"texto2.txt\", \"r\", encoding=\"utf8\")\n",
    "new_text = archivo_texto.read()\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "archivo_texto.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae095f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
